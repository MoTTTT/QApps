kubectl -n kube-system describe certificate kubernetes-dashboard-stg

dashboardview:$apr1$ml7f4qra$3l4yQY.c9Fn3vF400kL5c0

kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:kubernetes-dashboard

    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/secure-backends: "true"


kubectl -n kube-system logs -f $(kubectl -n kube-system get pods -l app=cert-manager -o jsonpath="{.items[0].metadata.name}") cert-manager
kube-system describe certificate kubernetes-dashboard-stg


kubectl -n legacy get secret wild.adyxax.org-cert -o json -o=jsonpath="{.data.tls\.crt}" | base64 -d > fullchain.cer
kubectl -n legacy get secret wild.adyxax.org-cert -o json -o=jsonpath="{.data.tls\.key}" | base64 -d > adyxax.org.key

kubectl logs  deployment/cert-manager  -n cert-manager

sudo microk8s kubectl describe secret -n kube-system microk8s-dashboard-token


Cert-manager is installed. As a next step, try creating a ClusterIssuer
for Let's Encrypt by creating the following resource:

$ microk8s kubectl apply -f - <<EOF
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
spec:
  acme:
    # You must replace this email address with your own.
    # Let's Encrypt will use this to contact you about expiring
    # certificates, and issues related to your account.
    email: me@example.com
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      # Secret resource that will be used to store the account's private key.
      name: letsencrypt-account-key
    # Add a single challenge solver, HTTP01 using nginx
    solvers:
    - http01:
        ingress:
          class: public
EOF

Then, you can create an ingress to expose 'my-service:80' on 'https://my-service.example.com' with:

$ microk8s enable ingress
$ microk8s kubectl create ingress my-ingress     --annotation cert-manager.io/cluster-issuer=letsencrypt     --rule 'my-service.example.com/*=my-service:80,tls=my-service-tls'


Kubectl logs podname -n namespace –all-containers=true.

kubectl logs ingress-nginx-controller-7ddd87655-t2btf --all-containers --namespace ingress-nginx 

kubectl logs cert-manager-5d6bc46969-cqxnw -n cert-manager –all-containers=true


helm show values ingress-nginx --repo https://kubernetes.github.io/ingress-nginx
kubectl get L2Advertisement --all-namespaces -o yaml
kubectl get ipaddresspool --all-namespaces -o yaml
kubectl -n ingress-nginx get svc

http://sigiriya:30862/

http://southern.podzone.net:30862/

ip link show
ifconfig | grep -m 1 "^[a-z0-9]*:" | sed -e's/\(^[a-z0-9]*\):.*$/\1/' | xargs -I {} sh -c "ethtool {}"
ifconfig | grep -m 1 "^[a-z0-9]*:" | sed -e's/\(^[a-z0-9]*\):.*$/\1/' | xargs -I {} sh -c "sudo tcpdump -i {}"

ip address show
ip route show
ss -tunap
sudo lsof -i @james

sudo microk8s kubectl config view --raw > $HOME/.kube/config

sudo snap install microk8s --channel=latest/edge/strict

 calicoctl --allow-version-mismatch  get node -o yaml

kubectl create deployment demo --image=httpd --port=80
kubectl expose deployment demo

kubectl scale deployment demo --replicas=0;
kubectl scale deployment demo --replicas=2;

/Users/martincolley/workspace/Certs/qsolutions/{cert1.pem, chain1.pem, fullchain1.pem, privkey1.pem}

kubectl create secret tls nginxsecret --key /Users/martincolley/workspace/Certs/qsolutions/privkey1.pem --cert /Users/martincolley/workspace/Certs/qsolutions/fullchain1.pem

kubectl create configmap nginxconfigmap --from-file=nginx-default.conf

 brew install calicoctl

grep -B 15 -A 15 IP_AUTODETECTION_METHOD /var/snap/microk8s/current/args/cni-network/cni.yaml


kubectl create ingress demo-localhost --class=nginx --rule="levant/*=demo:80"

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml

nmcli connection add type bridge con-name localbr ifname localbr ipv4.method manual ipv4.addresses 10.13.31.1/24

sudo microk8s kubectl config view --raw > $HOME/.kube/config
sudo usermod -a -G microk8s <username>

multipass launch --network en0 --network name=bridge0,mode=manual


ssh martin@bukit
ssh colleymj@sigiriya
ssh colleymj@james

kubectl cluster-info dump

kubectl get pods -n kube-system -o wide

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml


sudo snap remove microk8s

sudo snap install microk8s

microk8s config



ifconfig | grep -m 1 "^[a-z0-9]*:" | sed -e's/\(^[a-z0-9]*\):.*$/\1/' | xargs -I {} sh -c "ethtool {}"
arp -n
ip link show
route

ip route show



ifconfig | grep -m 1 "^[a-z0-9]*:" | sed -e's/\(^[a-z0-9]*\):.*$/\1/' | xargs -I {} sh -c "sudo tcpdump -i {}"

microk8s status

microk8s kubectl get all --all-namespaces -o wide
microk8s kubectl get pods -A

microk8s kubectl describe pvc my-pvc

sudo microk8s kubectl cluster-info

sudo microk8s kubectl get no -o yaml

microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 



### Command line snippets


sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a

sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause

sudo kubeadm init --control-plane-endpoint bukit --config kubeadm-config.yaml


/snap/docker/2893/bin/containerd config default

For snap docker installation (bukit):
sudo kubeadm init --cri-socket unix:/run/snap.docker/containerd/containerd.sock --control-plane-endpoint bukit --dry-run

For apt docker installation (james):
sudo kubeadm init --control-plane-endpoint bukit --dry-run

--config kubeadm-config.yaml

systemctl enable kubelet.service

kubectl version --client


cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter



cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system


### Deprecated

## Minimum Viable Product

- Containerise all services
- Manage services using k8s
- k8s cluster on premise
- Configuration management & governance

- Containerise all services
- Managed services, assets and processes
- Configuration management & governance



### Deprecated tasks




- Technical: High Availability, multi-site, consumer network, consumer hardware, opensource everything
- Technical: Secure site management from off-site
- EG: Log aggregation
- EG: On prem TLS
- EG: Multi-zone cluster: sothern, central, western

- Kubernetes host network IP range `--service-cluster-ip-range=192.168.0.128/25` in  `/var/snap/microk8s/current/args/kube-apiserver`


- `/var/snap/microk8s/current/args/cni-network/cni.yaml`

- edit `/var/snap/microk8s/current/args/kube-apiserver` and setting the following arguments:

```text
# /var/snap/microk8s/current/args/kube-apiserver
--advertise-address=10.10.10.10
--bind-address=0.0.0.0
--secure-port=16443
```
- CNI Pulgin: Calico, using 192.168.1.0/16: ```kubeadm init --pod-network-cidr=192.168.1.0/16```




### Tasklists: Levant

- [ ] Router: Static IP for levant MAC 
- [ ] Dolmen: Run RPi imager, set up wifi, ssh, hostname
- [ ] Levant: Prepend `cgroup_enable=memory cgroup_memory=1` to `/boot/firmware/cmdline.txt`


- [X] kubectl on bukit
- [X] kubectl on james
- [X] Confirm docker on bukit and james: run httpd
- [X] Create external etcd cluster: Removed from MVP

### k8s installation

- [X] bukit and james: Install kubeadm and kubelet: ```sudo apt-get install -y kubelet kubeadm```
- [X] bukit: Swap settings for kubeadm (sudo swapoff -a; comment out /swapfile in /etc/fstab)
- [X] bukit: Configure kubeadm for containerd  (create kubeadm-config.yaml)
- [ ] bukit: kubeadm init  --config kubeadm-config.yaml
- [ ] dolmen: Install kubectl
- [ ] Add third cluster node (virtual box on )



### Tasks: Cleanup to retry kubeadm init


### Tasks: Cleanup initial

- [X] james: snap cleanup
- [X] bukit: ```snap remove kube-apiserver kubectl```
- [X] james: Remove docker, kubectl, and k8s components
- [X] bukit: Remove docker, kubectl, and k8s components
- [X] bukit and james: Clean up k8s files (/var/lib/kubelet/; /etc/kubernetes/)
- [X] bukit and james: Clean up docker installation files ```rm -rf /var/lib/containerd``` and  ```rm -rf /var/lib/docker```
- [X] bukit and james: Clean up etcd files ```rm -rf /var/lib/etcd```
- [ ] bukit: ~/.kube and ~/.microk8s




### Tasks: Prep for k8s installation

- [X] bukit, dolmen and james: /etc/hosts file configs for james and bukit /private/etc/hosts for dolmen
- [X] james: Install containerd
- [X] bukit: Install containerd
- [X] james: Configure containerd (/etc/containerd/config.toml)
- [X] bukit: Configure containerd (/etc/containerd/config.toml)


## Containerd installation and configuration

Full docker stack, packaged by docker: ```sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin```

Just runc and containerd, packaged by ubuntu: ```apt-get -y install containerd```

Containerd configuration requirements:

- Enable cri plugin in /etc/containerd/config.toml (achieved by overwriting the default file config.toml)
- Set cgroup driver to Systemd
- Configure k8s sandbox image

The following achieves this:

```text
cat > /etc/containerd/config.toml <<EOF
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "registry.k8s.io/pause:3.9"
EOF
```

Restart containerd: ```systemctl restart containerd```

## kubeadm installation and configuration

- Set Pod subnet (--pod-network-cidr): 192.168.1.0/24
- Set Service subnet 192.168.0.0/24
- Set --control-plane-endpoint bukit
- Set systemd as the cgroup driver
- Since the config file option to init is required for cgroup driver, create a config file for all options:

```text
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.28.2
networking:
  serviceSubnet: "192.168.0.0/24"
  podSubnet: "192.168.1.0/24"
  dnsDomain: "cluster.local"
controlPlaneEndpoint: "bukit"
clusterName: "qapps-cluster"
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
```

- Init command: kubeadm init --config kubeadm-config.yaml

### Tasks: Cleanup to retdo kubeadm init

- kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets
- kubeadm reset 
- kubectl delete node <node name>

- clean /etc/cni/net.d

